{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importation des librairies**"
      ],
      "metadata": {
        "id": "F8lKLDEcN06K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0M-TSrke_P0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bcb0fc2-84a6-409e-8b1f-a1bc69c7f72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:18<00:00,  6.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données totales chargées: 2,099,994 lignes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEntraînement par lots:   0%|          | 0/17 [00:00<?, ?it/s][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    5.3s\n",
            "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   13.2s finished\n",
            "Entraînement par lots:   6%|▌         | 1/17 [00:13<03:31, 13.23s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n",
            "Entraînement par lots:  12%|█▏        | 2/17 [00:14<01:34,  6.30s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
            "Entraînement par lots:  18%|█▊        | 3/17 [00:15<00:55,  3.98s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
            "Entraînement par lots:  24%|██▎       | 4/17 [00:17<00:37,  2.86s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
            "Entraînement par lots:  29%|██▉       | 5/17 [00:18<00:27,  2.26s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.0s finished\n",
            "Entraînement par lots:  35%|███▌      | 6/17 [00:19<00:20,  1.85s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
            "Entraînement par lots:  41%|████      | 7/17 [00:20<00:16,  1.61s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.0s finished\n",
            "Entraînement par lots:  47%|████▋     | 8/17 [00:21<00:12,  1.44s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.0s finished\n",
            "Entraînement par lots:  53%|█████▎    | 9/17 [00:22<00:10,  1.32s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
            "Entraînement par lots:  59%|█████▉    | 10/17 [00:23<00:08,  1.28s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.8s finished\n",
            "Entraînement par lots:  65%|██████▍   | 11/17 [00:25<00:08,  1.44s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    2.2s finished\n",
            "Entraînement par lots:  71%|███████   | 12/17 [00:27<00:08,  1.69s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.4s finished\n",
            "Entraînement par lots:  76%|███████▋  | 13/17 [00:29<00:06,  1.61s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
            "Entraînement par lots:  82%|████████▏ | 14/17 [00:30<00:04,  1.47s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.1s finished\n",
            "Entraînement par lots:  88%|████████▊ | 15/17 [00:31<00:02,  1.38s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.2s finished\n",
            "Entraînement par lots:  94%|█████████▍| 16/17 [00:32<00:01,  1.34s/it][Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.9s finished\n",
            "Entraînement par lots: 100%|██████████| 17/17 [00:33<00:00,  1.98s/it]\n",
            "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rapport de classification final:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:    5.0s\n",
            "[Parallel(n_jobs=2)]: Done 260 out of 260 | elapsed:    8.6s finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99    373521\n",
            "           1       0.96      0.95      0.95     46478\n",
            "\n",
            "    accuracy                           0.99    419999\n",
            "   macro avg       0.98      0.97      0.97    419999\n",
            "weighted avg       0.99      0.99      0.99    419999\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/saved_models/network_forensics_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#-----------------------IMPORTATION\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "#-----------------------CHARGEMENT DES DATASETS\n",
        "DATA_DIR = '/content/drive/MyDrive/Dataset/'\n",
        "os.chdir(DATA_DIR)\n",
        "# Nomenclature des colonnes\n",
        "cols = [\n",
        "    'srcip','sport','dstip','dsport','proto','state','dur','sbytes','dbytes',\n",
        "    'sttl','dttl','sloss','dloss','service','Sload','Dload','Spkts','Dpkts',\n",
        "    'swin','dwin','stcpb','dtcpb','smeansz','dmeansz','trans_depth','res_bdy_len',\n",
        "    'Sjit','Djit','Stime','Ltime','Sintpkt','Dintpkt','tcprtt','synack','ackdat',\n",
        "    'is_sm_ips_ports','ct_state_ttl','ct_flw_http_mthd','is_ftp_login','ct_ftp_cmd',\n",
        "    'ct_srv_src','ct_srv_dst','ct_dst_ltm','ct_src_ltm','ct_src_dport_ltm',\n",
        "    'ct_dst_sport_ltm','ct_dst_src_ltm','attack_cat','label'\n",
        "]\n",
        "\n",
        "# Fonction de chargement du dataset\n",
        "def load_dataset(file):\n",
        "    try:\n",
        "        # Lecture du dataset avec les colonnes comme header\n",
        "        df = pd.read_csv(os.path.join(DATA_DIR, file), header=None, names=cols, low_memory=False)\n",
        "\n",
        "        # Tri des colonnes correspondantes aux logs Zeek\n",
        "        df = df[[\n",
        "            'srcip', 'dstip', 'sport', 'dsport', 'proto', 'sbytes', 'label'\n",
        "        ]]\n",
        "\n",
        "        # Renommage des colonnes\n",
        "        df.rename(columns={\n",
        "            'srcip': 'src_ip',\n",
        "            'dstip': 'dst_ip',\n",
        "            'sport': 'src_port',\n",
        "            'dsport': 'dst_port',\n",
        "            'proto': 'proto',\n",
        "            'sbytes': 'bytes'\n",
        "        }, inplace=True)\n",
        "\n",
        "        # Fonction de formatage\n",
        "        def parse_port(x):\n",
        "            try:\n",
        "                return int(str(x), 0)\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        df['src_port'] = df['src_port'].apply(parse_port)\n",
        "        df['dst_port'] = df['dst_port'].apply(parse_port)\n",
        "\n",
        "        df.dropna(subset=['src_port', 'dst_port'], inplace=True)\n",
        "\n",
        "        # Conversion\n",
        "        df = df.astype({\n",
        "            'src_ip': 'category',\n",
        "            'dst_ip': 'category',\n",
        "            'proto': 'category',\n",
        "            'src_port': 'uint16',\n",
        "            'dst_port': 'uint16',\n",
        "            'bytes': 'uint32',\n",
        "            'label': 'uint8'\n",
        "        })\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur avec {file}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "all_files = [f for f in os.listdir() if f.endswith('.csv')][:20]\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    datasets = list(\n",
        "        tqdm(executor.map(load_dataset, all_files), total=len(all_files)))\n",
        "\n",
        "full_data = pd.concat([d for d in datasets if d is not None], ignore_index=True)\n",
        "print(f\"Données totales chargées: {len(full_data):,} lignes\")\n",
        "\n",
        "#---------------PRETRAITEMENT DES DONNÉES\n",
        "# Fonction de pretraitement des données\n",
        "def preprocess_data(df):\n",
        "\n",
        "    cat_cols = ['proto', 'src_ip', 'dst_ip']\n",
        "    encoders = {}\n",
        "\n",
        "    for col in cat_cols:\n",
        "        if col in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "            encoders[col] = le\n",
        "\n",
        "    if 'bytes' in df.columns and 'duration' in df.columns:\n",
        "        df['bytes_per_sec'] = df['bytes'] / (df['duration'] + 1e-6)\n",
        "\n",
        "    features = ['src_port', 'dst_port', 'proto', 'bytes', 'bytes_per_sec']\n",
        "    features = [f for f in features if f in df.columns]\n",
        "\n",
        "    return df[features], df['label'], encoders\n",
        "\n",
        "X, y, encoders = preprocess_data(full_data)\n",
        "\n",
        "os.makedirs('preprocessing', exist_ok=True)\n",
        "for col, encoder in encoders.items():\n",
        "    joblib.dump(encoder, f'preprocessing/encoder_{col}.joblib')\n",
        "\n",
        "#---------------------ENTRAINEMENT DU MODELE\n",
        "# Séparation en features et labels de test et d'entrainement\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Normalisation\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "joblib.dump(scaler, 'preprocessing/scaler.joblib')\n",
        "\n",
        "# Entrainement du RandomForest\n",
        "batch_size = 100000\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    warm_start=True,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "for i in tqdm(range(0, len(X_train), batch_size), desc=\"Entraînement par lots\"):\n",
        "    batch_end = min(i + batch_size, len(X_train))\n",
        "    model.fit(X_train[i:batch_end], y_train[i:batch_end])\n",
        "    model.n_estimators += 10\n",
        "\n",
        "print(\"\\nRapport de classification final:\")\n",
        "print(classification_report(y_test, model.predict(X_test)))\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/saved_models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "joblib.dump(model, os.path.join(model_dir, 'network_forensics_model.joblib'))"
      ]
    }
  ]
}